{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<center> <h1>Unit 5</h1> </center>\n",
    "<center> <h1>Introduction to Apache Spark</h1> </center>\n",
    "<br>\n",
    "<br>\n",
    "<br>\n",
    "<center> <h3>IST 718 – Big Data Analytics</h3> </center>\n",
    "<center> <h3>Daniel E. Acuna</h3> </center>\n",
    "<center> <h3>http://acuna.io</h3> </center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Objectives:  \n",
    "\n",
    "- Describe Apache Spark.  \n",
    "\n",
    "- List Spark components and their functionalities.  \n",
    "\n",
    "- Use Resilient Distributed Datasets.  \n",
    "\n",
    "- Describe the Spark MLlib.  \n",
    "\n",
    "- Perform Spark Operations."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Typical data science cycle\n",
    "<br>\n",
    "<center><img src=\"./images/unit-05/unit-05-0_as1.png\" width=\"75%\" align=\"center\"></center>\n",
    "<br>\n",
    "<center>Fom IBM’s Foundational Methdology for Data Science</center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Typical data science cycle (2)\n",
    "<br>\n",
    "<center><img src=\"./images/unit-05/unit-05-0_as2.png\" width=\"85%\" align=\"center\"></center>\n",
    "<br>  \n",
    "<center>Fom IBM’s Foundational Methdology for Data Science</center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Example\n",
    "- Log files visit to a website – Apache Logs:  \n",
    "\n",
    "<br>\n",
    "<center><img src=\"./images/unit-05/unit-05-0_as3.png\" width=\"90%\" align=\"center\"></center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Example (2)\n",
    "<div class=\"container2\">\n",
    "  <div class=\"row2\">\n",
    "    <div class=\"col-12\">\n",
    "        <img src=\"./images/unit-05/unit-05-0_as4.png\" width=\"80%\" align=\"right\">\n",
    "    </div>\n",
    "  </div>\n",
    "</div>\n",
    "<br>\n",
    "<div class=\"container2\">\n",
    "  <div class=\"row2\">\n",
    "    <div class=\"col-6\">\n",
    "      <left><img src=\"./images/unit-05/unit-05-0_as5.png\" width=\"100%\" align=\"left\"></left>\n",
    "    </div>    \n",
    "    <div class=\"col-6  right2\">\n",
    "      <ol>\n",
    "        <li>Business question based on this data?</li>\n",
    "        <li>Data understanding steps?</li>\n",
    "        <li>Data preparation?</li>\n",
    "      </ol>\n",
    "    </div>\n",
    "  </div>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Example (3)\n",
    "- Let’s assume we want to predict whether time of the day and month predicts the type of response:  \n",
    "<br>\n",
    "<center><img src=\"./images/unit-05/unit-05-0_as4.png\" width=\"100%\" align=\"center\"></center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Example (4)\n",
    "- With Apache Spark we will see how to take the **raw input data** and transform it non-destructively to clean it up or analyze it.  \n",
    "<br>\n",
    "<center><img src=\"./images/unit-05/unit-05-0_as7.png\" width=\"100%\" align=\"center\"></center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Hadoop\n",
    "Traditionally:\n",
    "- Hadoop uses single programming model: MapReduce.\n",
    "- It only works on hard drives.  \n",
    "<br>\n",
    "<center><img src=\"./images/unit-05/unit-05-0_as8.png\" width=\"100%\" align=\"center\"></center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Spark\n",
    "- RAM bandwidth has been increasing exponentially.\n",
    "- Spark can perform in-memory computations. \n",
    "\n",
    "<br>\n",
    "<center><img src=\"./images/unit-05/unit-05-0_as9.png\" width=\"60%\" align=\"center\"></center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# What is Spark?\n",
    "- **Apache Spark** is a fast, in-memory analytics system.\n",
    "- Spark has several high-level tools, including:\n",
    "  - **ML**: a machine learning library.\n",
    "  - **Spark Streaming**: enables high-throughput, fault-tolerant stream processing of live data streams.\n",
    "  - **Spark SQL**: runs SQL and HiveQL queries.\n",
    "  - **GraphX**: an API for graphs and graph-parallel computation.\n",
    "- Spark can be executed in two ways:\n",
    "  - Independent processes on a cluster.\n",
    "  - As a YARN application.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Spark\n",
    "<div class=\"blockquote2\">\n",
    "Apache Spark™ is a fast and general engine for large-scale data processing.\n",
    "</div>\n",
    "<br>\n",
    "<center><img src=\"./images/unit-05/unit-05-0_as10_2.png\" width=\"70%\" align=\"center\"></center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Spark Architectural Components\n",
    "<center><img src=\"./images/unit-05/unit-05-0_as11.png\" width=\"45%\" align=\"center\"></center>\n",
    "- Spark applications define various transformations and actions on data, but the actual steps of the script are not executed until an output is requested. \n",
    "- The Spark application can run on different cluster managers such as local, Yarn, Mesos, and Kubernetes."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Spark 1.6+ vs 2.0+\n",
    "- Spark 1.6+ relied on transformations on arbitrary datasets known as Resilient Distributed Datasets (RDDs.)  \n",
    "\n",
    "- With more flexiblity comes greater power but less performance.  \n",
    "\n",
    "- Spark 2.0+ defines more structure in the form of `DataFrame`, which is similar to a table in SQL and `data.frame` in R. \n",
    "\n",
    "- The newest versions of Spark define `DataSet` which is statically-typed `DataFrame` (more structure.)  \n",
    "\n",
    "- Future Spark ML will only work with DataFrames and not RDDs."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Create Spark session"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "# Spark 2.0+\n",
    "#Access Dataframes\n",
    "spark = SparkSession.builder.getOrCreate()\n",
    "# Spark 1.6 (RDDs)\n",
    "sc = spark.sparkContext"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Classic Spark v. <2.0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# The SparkContext\n",
    "\n",
    "- The **SparkContext** object performs the following tasks:  \n",
    "\n",
    "  - It connects to the YARN ResourceManager and asks for resources on the Hadoop cluster,  \n",
    "  \n",
    "  - Starts executors on the worker nodes in the cluster that the ResourceManager allocated for the Spark application,  \n",
    "  \n",
    "  - Sends the application code to the executors,  \n",
    "  \n",
    "  - And finally, it sends tasks for the executors to run.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Spark RDDs\n",
    "- An RDD (resilient distributed dataset) is a fault-tolerant collection of elements that can be operated on in parallel.  \n",
    "\n",
    "- An RDD is an immutable collection that represents:\n",
    "  - A dataset…\n",
    "  - …broken up into a list of partitions.\n",
    "  - A list of dependencies on other RDDs.\n",
    "  - An optional list of preferred block locations for an HDFS file.\n",
    "  - Read-only"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Important RDD Concepts\n",
    "- **Lineage**:\n",
    "  - Information about how an RDD was derived from other datasets or other RDDs.\n",
    "  - RDD is not necessarily materialized all the time.\n",
    "  - Lineage captured on disk as \"lineage graph.\"  \n",
    "  \n",
    "- **Persistence**:\n",
    "  - Indicate which RDDs they want to keep in memory. \n",
    "  - User can call \"persist\" method.  \n",
    "  \n",
    "- **Partitioning**:\n",
    "  - RDD elements can be partitioned across machines based on a key in each record.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Creating RDDs\n",
    "- Can create an initial RDD by applying a transformation to data on disk.  \n",
    "\n",
    "- Can create an initial RDD from a code object.  \n",
    "\n",
    "- Example ways to create an RDD in Spark:\n",
    "  - Use the `parallelize` operation to convert an existing code object into an RDD.\n",
    "  - Use `textFile` operation to convert a text file on HDFS into an RDD.\n",
    "  - Use `sequenceFile` operation to convert a binary file on HDFS into an RDD."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Example: Creating RDDs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19]"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "myarray = list(range(1, 20))\n",
    "myarray"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ParallelCollectionRDD[0] at readRDDFromFile at PythonRDD.scala:262"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dist_array = sc.parallelize(myarray) # parallelize version of myarray\n",
    "dist_array # it doesn't do anything"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# RDD Persist\n",
    "- Spark’s persist method:\n",
    "  - Indicates which RDDs to reuse.\n",
    "  - Indicates if you want to replicate across machines.\n",
    "  - Indicates priority of which in-memory data to spill to disk first.  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "shakespeare = sc.textFile(\"../notebooks/datasets/shakespeare.txt\")\n",
    "love = shakespeare.filter(lambda x: 'love' in x.lower())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "160 ms ± 23.4 ms per loop (mean ± std. dev. of 7 runs, 1 loop each)\n"
     ]
    }
   ],
   "source": [
    "%timeit love.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2653"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "love.cache()\n",
    "# no effect first time\n",
    "love.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "82.6 ms ± 3.8 ms per loop (mean ± std. dev. of 7 runs, 10 loops each)\n"
     ]
    }
   ],
   "source": [
    "%timeit love.count()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# RDD Disk and Recover\n",
    "- RDD can spill to disk:\n",
    "  - Degrades gracefully (to mapreduce performance.)\n",
    "  - Partitions not in use/lesser use and/or low priority spilled first.  \n",
    "  \n",
    "- Failure Recovery:\n",
    "  - RDD partition that fails are recovered by Yarn/Spark Driver.\n",
    "  - Executer applies lineage to prior RDD (or original data on disk) for that partition.\n",
    "  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# RDD Disk and Recover (2)\n",
    "- Dependencies: \n",
    "  - Narrow – child partition depends on only one parent partition.\n",
    "    - e.g. `map`, `filter`, `union`, HDFS blocks transformations.\n",
    "  - Wide – multiple child partitions depend on one parent partition.\n",
    "    - e.g. `join` and `groupBy` transformations.\n",
    "    - Materialize intermediate calculations on parent for fault recovery.\n",
    "- Checkpointing:\n",
    "  - Materialize RDD with long lineage."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# RDD Operations\n",
    "- There are two types of operations that can be done on RDDs:\n",
    "  - **Transformations**: create a new dataset/RDD from an existing one.\n",
    "  - **Actions**: which return a value to the driver program after running a computation on the RDD.  \n",
    "  \n",
    "- Transformations:\n",
    "  - Lazy - they do not compute their results right away.\n",
    "  - \"Coarse-grain\"/Bulk only – no cell level updates.  \n",
    "  \n",
    "- Actions:\n",
    "  - Make RDD materialize and run on cluster."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Transformations\n",
    "- `map(func)`: returns a new distributed dataset formed by passing each element of the source through the function *func*.\n",
    "- `flatMap(func)`: same as `map` but when multiple key-value pairs are returned.\n",
    "- `filter(func)`: return a new dataset formed by selecting those elements of the source on which *func* returns true. \n",
    "- `reduceByKey(func)`: when called on a dataset of (K, V) pairs, returns a dataset of (K, V) pairs where the values for each key are aggregated using the given reduce function *func*.\n",
    "- `sortByKey([ascending],[numTasks])`: when called on a dataset of (K, V) pairs where K implements `Ordered`, returns a dataset of (K, V) pairs sorted by keys in ascending or descending order, as specified in the Boolean **ascending** argument.\n",
    "- `join(otherDataset, [numTasks])`: when called on datasets of type (K, V) and (K, W), returns a dataset of (K, (V, W)) pairs with all pairs of elements for each key.\n",
    "- `distinct([numTasks])`: returns a new dataset that contains the distinct elements of the source dataset.\n",
    "- `pipe(command, [envVars])`: pipes each partition of the RDD through the provided shell *command*.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Example: Transformations  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "neg_values [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18]\n",
      "large_values [11, 12, 13, 14, 15, 16, 17, 18, 19]\n"
     ]
    }
   ],
   "source": [
    "neg_values = dist_array.map(lambda x: x-1)\n",
    "print(\"neg_values\", neg_values.collect())\n",
    "large_values = dist_array.filter(lambda y: y > 10)\n",
    "print(\"large_values\", large_values.collect())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Actions\n",
    "- `reduce(func`): Aggregate the elements of the dataset using a function *func* (which takes two arguments and returns one). The function should be commutative and associative so that it can be computed correctly in parallel.\n",
    "- `foreach(func)`: Runs the function *func* on each element in the dataset.\n",
    "- `count()`: returns the number of elements in the dataset.\n",
    "- `first()`: returns the first element in the dataset.\n",
    "- `take(n)`: returns an array with the first ***n*** elements of the dataset.\n",
    "- `saveAsTextFile(path)`: writes the elements in the dataset out to a file in HDFS (or some other file system.)\n",
    "- `saveAsSequenceFile(path)`: writes the elements to HDFS in the `SequenceFile` format."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Example: Actions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[11, 12, 13, 14, 15, 16, 17, 18, 19]"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "large_values = dist_array.filter(lambda y: y > 10)\n",
    "large_values.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "pyspark.rdd.PipelinedRDD"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(large_values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "9"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "large_values.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "!rm -rf /tmp/large_values.txt\n",
    "large_values.saveAsTextFile('/tmp/large_values.txt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "part-00000  part-00001  part-00002  part-00003  _SUCCESS\r\n"
     ]
    }
   ],
   "source": [
    "%ls /tmp/large_values.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "cat: /tmp/large_values.txt/part-00006: No such file or directory\n"
     ]
    },
    {
     "ename": "CalledProcessError",
     "evalue": "Command 'b'cat /tmp/large_values.txt/part-00006\\n'' returned non-zero exit status 1.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mCalledProcessError\u001b[0m                        Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-16-4c6ccbed94c8>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mget_ipython\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun_cell_magic\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'bash'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m''\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'cat /tmp/large_values.txt/part-00006\\n'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/opt/conda/lib/python3.8/site-packages/IPython/core/interactiveshell.py\u001b[0m in \u001b[0;36mrun_cell_magic\u001b[0;34m(self, magic_name, line, cell)\u001b[0m\n\u001b[1;32m   2380\u001b[0m             \u001b[0;32mwith\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbuiltin_trap\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2381\u001b[0m                 \u001b[0margs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mmagic_arg_s\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcell\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2382\u001b[0;31m                 \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2383\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2384\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.8/site-packages/IPython/core/magics/script.py\u001b[0m in \u001b[0;36mnamed_script_magic\u001b[0;34m(line, cell)\u001b[0m\n\u001b[1;32m    140\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    141\u001b[0m                 \u001b[0mline\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mscript\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 142\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshebang\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mline\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcell\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    143\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    144\u001b[0m         \u001b[0;31m# write a basic docstring:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<decorator-gen-103>\u001b[0m in \u001b[0;36mshebang\u001b[0;34m(self, line, cell)\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.8/site-packages/IPython/core/magic.py\u001b[0m in \u001b[0;36m<lambda>\u001b[0;34m(f, *a, **k)\u001b[0m\n\u001b[1;32m    185\u001b[0m     \u001b[0;31m# but it's overkill for just that one bit of state.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    186\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mmagic_deco\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0marg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 187\u001b[0;31m         \u001b[0mcall\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mlambda\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mk\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mk\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    188\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    189\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mcallable\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0marg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.8/site-packages/IPython/core/magics/script.py\u001b[0m in \u001b[0;36mshebang\u001b[0;34m(self, line, cell)\u001b[0m\n\u001b[1;32m    243\u001b[0m             \u001b[0msys\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstderr\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mflush\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    244\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mraise_error\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreturncode\u001b[0m\u001b[0;34m!=\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 245\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0mCalledProcessError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreturncode\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcell\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moutput\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mout\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstderr\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0merr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    246\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    247\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_run_script\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mp\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcell\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mto_close\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mCalledProcessError\u001b[0m: Command 'b'cat /tmp/large_values.txt/part-00006\\n'' returned non-zero exit status 1."
     ]
    }
   ],
   "source": [
    "%%bash\n",
    "cat /tmp/large_values.txt/part-00006"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# WordCount in Spark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'shakespeare' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-11-f0c1b89b50c7>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mwordCounts\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mshakespeare\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mflatMap\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;32mlambda\u001b[0m \u001b[0mline\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mline\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlower\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;31m\\\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m     \u001b[0mmap\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;32mlambda\u001b[0m \u001b[0mword\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mword\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;31m\\\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m     \u001b[0mreduceByKey\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;32mlambda\u001b[0m \u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mb\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0ma\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mb\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;31m\\\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m     \u001b[0msortBy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;32mlambda\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;34m-\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mwordCounts\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtake\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m10\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'shakespeare' is not defined"
     ]
    }
   ],
   "source": [
    "wordCounts = shakespeare.flatMap(lambda line: line.lower().split()).\\\n",
    "    map(lambda word: (word, 1)).\\\n",
    "    reduceByKey(lambda a, b: a + b).\\\n",
    "    sortBy(lambda x: -x[1])\n",
    "wordCounts.take(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Operations on key-value pair RDDs\n",
    "- Operations on key-value pair datasets are at the foundation of Hadoop, Spark 1.6, and MapReduce.\n",
    "  - `groupByKey()`: group values with the same key, ex. `rdd.groupByKey()`.\n",
    "  - `mapValues(f)`: applies function only to values not keys.\n",
    "  - `flatMapValues(f)`: same as `mapValues` when function returns several values.\n",
    "  - `keys()`: RDD with only the keys.\n",
    "  - `values()`: RDD with only the values.  \n",
    "  \n",
    "- Simple operations on RDDs:\n",
    "  - `union(otherRDD)`: makes the union of all key-value pairs."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Join operations on pairs of key-value pair RDDs: join operations\n",
    "- Joins are a fundamental operations of data which compute the **Cartesian Product** between two sets (e.g., two RDDs):\n",
    "$$A \\times B=\\{(a,b) \\mid a \\in A \\wedge b \\in B\\}$$  \n",
    "\n",
    "- Most of the time, joins are paired with a filter to improve performance.  \n",
    "<br>\n",
    "<center><img src=\"./images/unit-05/unit-05-0_as17.png\" width=\"50%\" align=\"center\"></center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Join operations on pairs of key-value pair RDDs: join operations (2)\n",
    "- We can use this idea to join key-value pairs and then filter for pairs that have the same key."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "A = sc.parallelize([(1, 1), (1, 2), (2, 3)])\n",
    "B = sc.parallelize([(1, \"A\"), (2, \"B\")])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "A.join(B).collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Join operations on pairs of key-value pair RDDs: example\n",
    "- Given the following dataset, compute the total number of orders per state\n",
    "  - Locations: locationID, state.\n",
    "  - Transactions: transactionID, locationID, number of orders.\n",
    "- Explore `leftOuterJoin` and `rightOuterJoin`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "locations = sc.parallelize([('loc1', 'NY'), ('loc2', 'NY'), \n",
    "                            ('loc3', 'PA'), ('loc4', 'FL')])\n",
    "transactions = sc.parallelize([(1, 'loc1', 2), (2, 'loc1', 3), \n",
    "                               (3, 'loc2', 5), (4, 'loc5', 5)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "locations = sc.parallelize([('loc1', 'NY'), ('loc2', 'NY'), \n",
    "                            ('loc3', 'PA'), ('loc4', 'FL')])\n",
    "transactions = sc.parallelize([(1, 'loc1', 2), (2, 'loc1', 3), \n",
    "                               (3, 'loc2', 5), (4, 'loc5', 5)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('loc1', ('NY', 2)), ('loc1', ('NY', 3)), ('loc2', ('NY', 5))]"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "join_loc_tr = locations.\\\n",
    "    join(transactions.map(lambda x: x[1:]))\n",
    "join_loc_tr.take(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('NY', 10)]"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "join_loc_tr.values().\\\n",
    "    reduceByKey(lambda v1, v2: v1+v2).\\\n",
    "    collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('loc1', ('NY', 2)),\n",
       " ('loc1', ('NY', 3)),\n",
       " ('loc3', ('PA', None)),\n",
       " ('loc2', ('NY', 5)),\n",
       " ('loc4', ('FL', None))]"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "locations.\\\n",
    "    leftOuterJoin(transactions.map(lambda x: x[1:])).collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "locations.\\\n",
    "    rightOuterJoin(transactions.map(lambda x: x[1:])).collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "locations.\\\n",
    "    fullOuterJoin(transactions.map(lambda x: x[1:])).collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Spark 2.0\n",
    "### <span style=\"color:gray\">DataFrames</span>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# DataFrames\n",
    "- The problem with RDDs is that they do not have enough structure.  \n",
    "\n",
    "- They are harder to optimize and therefore slow.  \n",
    "\n",
    "- DataFrames aim at solving this by adding structure.  \n",
    "\n",
    "- A DataFrame is a distributed collection of data organized into named columns.  \n",
    "\n",
    "- Similar to Pandas DataFrames but distributed across the cluster.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# DataFrames (2)\n",
    "- You can read from multiple sources into dataframes.  \n",
    "<br>\n",
    "<center><img src=\"./images/unit-05/unit-05-0_as20.png\" width=\"100%\" align=\"center\"></center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# DataFrames (3)\n",
    "- One preferred source is Parquet files.\n",
    "- Parquet files are datasets store in columns.  \n",
    "<br>\n",
    "<center><img src=\"./images/unit-05/unit-05-0_as21.png\" width=\"100%\" align=\"center\"></center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# DataFrame operations\n",
    "Creation of dataframes:\n",
    "- From Row Python objects:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql import Row\n",
    "raw_data = [Row(state='NY', month='JAN', orders=3),\n",
    "            Row(state='NJ', month='JAN', orders=4),\n",
    "            Row(state='NY', month='FEB', orders=5)\n",
    "           ]\n",
    "data_df = spark.createDataFrame(raw_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Row(state='NY', month='JAN', orders=3)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_df.first()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "- From RDDs:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[state: string, month: string, orders: bigint]"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rows_rdd = sc.parallelize(raw_data)\n",
    "rows_rdd.toDF()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "- From files:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[StreetNumber: string, StreetName: string, StreetNamePostType: string, Directional: string, strLocation: string, dtTime: string, streetID: string, VehicleName: string, Latitude: string, Longitude: string]"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark.read.csv('../notebooks/datasets/potholes_2016.csv', header=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# DataFrame operations (2)\n",
    "- Each column of a DataFrame has the same type.  \n",
    "\n",
    "- DataFrame types can be hierarchical: a column might be a \"DataFrame\" itself.  \n",
    "\n",
    "- **You no longer operate using Python**.  \n",
    "\n",
    "- Instead, you transform DataFrames by **selecting**, **modifying**, **filtering**, **joining**, **grouping**, or **aggregating** using specialized commands similar to SQL.  \n",
    "\n",
    "- Many of these commands are **symbolic representations** of the operations.  \n",
    "\n",
    "- After the transformations, Spark builds an *execution plan* that is optimized for the column types and the operations."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Exploring a DataFrame\n",
    "- `printSchema()`: shows the datatypes of the dataframe.  \n",
    "\n",
    "- `show()`: prints the first $n$ rows.  \n",
    "\n",
    "- `take()`: return first $n$ of rows.  \n",
    "\n",
    "- `sample(withReplacement, fraction)`: randomly sample rows (approximate.)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Selecting and modifying a DataFrame\n",
    "- `select(*expressions)`: returns a new DataFrame with columns.  \n",
    "\n",
    "- `withColumn(colName, expression)`: creates a new column based on the expression.  \n",
    "\n",
    "- Expressions are **symbolic operations**.  \n",
    "\n",
    "- Symbolic operations can hold *literal* values and *placeholders* for columns.  \n",
    "\n",
    "- You can perform symbolic operations on both literals and placeholders.  \n",
    "\n",
    "- Some symbolic operations are available in the module `pyspark.sql.functions`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Example Dataframes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "sc = spark.sparkContext\n",
    "from pyspark.sql import functions as fn\n",
    "\n",
    "locations_df = spark.createDataFrame([\n",
    "    Row(location_id = 'loc1', n_employees=3, state='NY'),\n",
    "    Row(location_id = 'loc2', n_employees=8, state='NY'),\n",
    "    Row(location_id = 'loc3', n_employees=3, state='PA'),\n",
    "    Row(location_id = 'loc4', n_employees=1, state='FL')    \n",
    "])\n",
    "transactions_df = spark.createDataFrame([\n",
    "    Row(transaction_id = 1, location_id = 'loc1', n_orders=2.),\n",
    "    Row(transaction_id = 2, location_id = 'loc1', n_orders=3.),\n",
    "    Row(transaction_id = 3, location_id = 'loc3', n_orders=5.),\n",
    "    Row(transaction_id = 4, location_id = 'loc5', n_orders=5.)\n",
    "])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Symbolic operations\n",
    "<br>\n",
    "<div class=\"container2\">\n",
    "  <div class=\"row2\">\n",
    "    <div class=\"col-6\">\n",
    "      <ul>\n",
    "        <li>expression =  1 + n_employees</li>\n",
    "        <br>            \n",
    "        <li>To express the previous operation, we need a *literal* value (`1`) and a *placeholder* for the column `n_employees`:</li>\n",
    "        <br>  \n",
    "        <center><img src=\"./images/unit-05/unit-05-0_as25.png\" width=\"80%\" align=\"center\"></center>\n",
    "        <br>\n",
    "        <li>A symbolic operation produces a *column* which itself has multiple operations</li>\n",
    "      </ul>\n",
    "    </div>\n",
    "  <div class=\"col-6\">\n",
    "    <center><img src=\"./images/unit-05/unit-05-0_as26.png\" width=\"70%\" align=\"center\"></center>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Symbolic operations (2)\n",
    "<br>\n",
    "<center><img src=\"./images/unit-05/unit-05-0_as27.png\" width=\"100%\" align=\"center\"></center>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------------+\n",
      "|(n_employees + 1)|\n",
      "+-----------------+\n",
      "|                4|\n",
      "|                9|\n",
      "|                4|\n",
      "|                2|\n",
      "+-----------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "locations_df.select(1 + fn.col('n_employees')).show(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+\n",
      "|new_column|\n",
      "+----------+\n",
      "|         4|\n",
      "|         9|\n",
      "|         4|\n",
      "|         2|\n",
      "+----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "new_column = 1 + fn.col('n_employees')\n",
    "locations_df.select(new_column.alias('new_column')).show(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "locations_df.select(new_column.alias('new_column').cast('float')).show(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Selecting and modifying\n",
    "- You can use `select` to subselect columns, modify them, or create new columns.   \n",
    "<br>\n",
    "<center><img src=\"./images/unit-05/unit-05-0_as28.png\" width=\"100%\" align=\"center\"></center>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+-----------+-----+------------------+----------------+\n",
      "|n_employees|location_id|state|n_employees_plus_1|more_than_5_empl|\n",
      "+-----------+-----------+-----+------------------+----------------+\n",
      "|          3|       loc1|   NY|                 4|           false|\n",
      "|          8|       loc2|   NY|                 9|            true|\n",
      "|          3|       loc3|   PA|                 4|           false|\n",
      "|          1|       loc4|   FL|                 2|           false|\n",
      "+-----------+-----------+-----+------------------+----------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "locations_df.\\\n",
    "    select('n_employees',\n",
    "          'location_id',\n",
    "          'state',\n",
    "          (fn.col('n_employees') + 1).alias('n_employees_plus_1'),\n",
    "          (fn.col('n_employees') > 5).alias('more_than_5_empl')\n",
    "          ).\\\n",
    "    show(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Selecting and modifying (2)\n",
    "- Some operations cannot receive a string representing a column so you must explicitly pass a placeholder or a literal.  \n",
    "<br>\n",
    "<center><img src=\"./images/unit-05/unit-05-0_as29.png\" width=\"60%\" align=\"center\"></center>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------------+-----------+---------------------+\n",
      "| SQRT(n_employees)|n_employees|POWER(n_employees, 2)|\n",
      "+------------------+-----------+---------------------+\n",
      "|1.7320508075688772|          3|                  9.0|\n",
      "|2.8284271247461903|          8|                 64.0|\n",
      "|1.7320508075688772|          3|                  9.0|\n",
      "|               1.0|          1|                  1.0|\n",
      "+------------------+-----------+---------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "locations_df.\\\n",
    "    select(fn.sqrt('n_employees'),\n",
    "           'n_employees',\n",
    "           fn.pow(fn.col('n_employees'), fn.lit(2))\n",
    "          ).\\\n",
    "    show()           "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Filtering\n",
    "- `where(expression)`: select only rows where expression is true.  \n",
    "\n",
    "- Expressions can be complex:\n",
    "<br>\n",
    "<center><img src=\"./images/unit-05/unit-05-0_as30.png\" width=\"100%\" align=\"center\"></center>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+-----------+-----+\n",
      "|location_id|n_employees|state|\n",
      "+-----------+-----------+-----+\n",
      "|       loc3|          3|   PA|\n",
      "+-----------+-----------+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "locations_df.where((fn.col('n_employees') > 2) & \\\n",
    "                  (fn.col('state') == 'PA')).\\\n",
    "         show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Joining\n",
    "<center><img src=\"./images/unit-05/unit-05-0_as31.png\" width=\"90%\" align=\"center\"></center>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "new_df = locations_df.join(transactions_df, on='location_id')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "== Physical Plan ==\n",
      "*(5) Project [location_id#48, n_employees#49L, state#50, transaction_id#54L, n_orders#56]\n",
      "+- *(5) SortMergeJoin [location_id#48], [location_id#55], Inner\n",
      "   :- *(2) Sort [location_id#48 ASC NULLS FIRST], false, 0\n",
      "   :  +- Exchange hashpartitioning(location_id#48, 200), true, [id=#90]\n",
      "   :     +- *(1) Filter isnotnull(location_id#48)\n",
      "   :        +- *(1) Scan ExistingRDD[location_id#48,n_employees#49L,state#50]\n",
      "   +- *(4) Sort [location_id#55 ASC NULLS FIRST], false, 0\n",
      "      +- Exchange hashpartitioning(location_id#55, 200), true, [id=#96]\n",
      "         +- *(3) Filter isnotnull(location_id#55)\n",
      "            +- *(3) Scan ExistingRDD[transaction_id#54L,location_id#55,n_orders#56]\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# you can look at the execution plan\n",
    "new_df.explain()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Joining (left outer join)\n",
    "<center><img src=\"./images/unit-05/unit-05-0_as32.png\" width=\"90%\" align=\"center\"></center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Joining (right outer join)\n",
    "<center><img src=\"./images/unit-05/unit-05-0_as33.png\" width=\"90%\" align=\"center\"></center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Joining (outer join)\n",
    "<center><img src=\"./images/unit-05/unit-05-0_as34.png\" width=\"90%\" align=\"center\"></center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Grouping\n",
    "- `groupBy(*expressions)`: groups by a list of expressions.  \n",
    "\n",
    "- Typically, a list of columns:\n",
    "<center><img src=\"./images/unit-05/unit-05-0_as35.png\" width=\"70%\" align=\"center\"></center>  \n",
    "\n",
    "\n",
    "- This does not do anything until we aggregate the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "locations_df.join(transactions_df, on='location_id').\\\n",
    "    groupBy('state')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Aggregate\n",
    "- There are some special functions that only work on grouped data.  \n",
    "\n",
    "- They are applied using the method `agg(*expressions)`.  \n",
    "\n",
    "- For example:\n",
    "  - `fn.sum`, `fn.stddev`: self explanatory.\n",
    "  - `fn.count`: counts when column is not null.\n",
    "  - `fn.countDistinct`: how many distinct values of a column.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Aggregate (2)\n",
    "<br>\n",
    "<center><img src=\"./images/unit-05/unit-05-0_as36.png\" width=\"60%\" align=\"center\"></center>  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'locations_df' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-7-03dfea4c20b7>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mlocations_df\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtransactions_df\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mon\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'location_id'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;31m\\\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m     \u001b[0mgroupBy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'state'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;31m\\\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m     \u001b[0magg\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'n_orders'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;31m\\\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m     \u001b[0mshow\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'locations_df' is not defined"
     ]
    }
   ],
   "source": [
    "locations_df.join(transactions_df, on='location_id').\\\n",
    "    groupBy('state').\\\n",
    "    agg(fn.sum('n_orders')).\\\n",
    "    show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Sampling from DataFrame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+-----------+-----+\n",
      "|location_id|n_employees|state|\n",
      "+-----------+-----------+-----+\n",
      "|       loc4|          1|   FL|\n",
      "+-----------+-----------+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "locations_df.sample(withReplacement=True, fraction=1.).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "# get result as Pandas\n",
    "locations_df.limit(2).toPandas()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Random number generation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "spark.range(start=0, end=100000).show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "spark.range(start=0, end=100000).\\\n",
    "    select('id', \n",
    "           fn.randn().alias('gaussian_sample'), \n",
    "           fn.rand().alias('uniform_sample')).show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Spark ML\n",
    "- Spark ML implements several machine learning algorithms at scale:\n",
    "  - Regression\n",
    "  - Classification\n",
    "  - Recommendation systems  \n",
    "  \n",
    "- It works on Spark DataFrames by performing transformations of the data.  \n",
    "\n",
    "- A typical data science analysis requires several transformations.  \n",
    "\n",
    "- These transformation can be implemented through Pipelines.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Spark ML (2)\n",
    "- A model is known as **Estimator** in Spark ML and the typical cycle for such objects is as follows:\n",
    "  1. Define zero or more **input columns**.\n",
    "  2. Define zero or more **output columns**.\n",
    "  3. Define **parameters** of the estimator.\n",
    "  4. **Fit** the estimator, which returns a **fitted model**.\n",
    "  5. Use the fitted model to perform **transformations**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Spark ML: Transformers and Estimators\n",
    "- Notice that Spark ML for RDD is getting deprecated.\n",
    "- Preferred method in the future is to work only with DataFrames.\n",
    "- Spark Machine Learning works by creating transformers and estimators and takes as input DataFrames.  \n",
    "\n",
    "<br>\n",
    "<center><img src=\"./images/unit-05/unit-05-0_as37.png\" width=\"100%\" align=\"center\"></center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Spark ML: Transformers and Estimators (2)\n",
    "- Estimators **need to learn something from the data**.\n",
    "- For example, if we want to count terms in text, we **need to know how many terms are in all documents**.  \n",
    "\n",
    "<br>\n",
    "<center><img src=\"./images/unit-05/unit-05-0_as38.png\" width=\"100%\" align=\"center\"></center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Linear regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "# regression\n",
    "from pyspark.sql import SparkSession\n",
    "sc = spark.sparkContext\n",
    "from pyspark.ml import regression\n",
    "# feature engineering\n",
    "from pyspark.ml import feature\n",
    "\n",
    "lr_estimator = regression.\\\n",
    "    LinearRegression(featuresCol=\"feature_column\", \n",
    "                     labelCol='label_column')\n",
    "# to fit I would need a proper feature column with a vector\n",
    "# lr_transformer = lr_estimator.fit(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Linear regression (2)\n",
    "- Suppose we wanted to predict number of orders from number of employees"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>location_id</th>\n",
       "      <th>n_employees</th>\n",
       "      <th>state</th>\n",
       "      <th>transaction_id</th>\n",
       "      <th>n_orders</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>loc1</td>\n",
       "      <td>3</td>\n",
       "      <td>NY</td>\n",
       "      <td>1</td>\n",
       "      <td>2.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>loc1</td>\n",
       "      <td>3</td>\n",
       "      <td>NY</td>\n",
       "      <td>2</td>\n",
       "      <td>3.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>loc3</td>\n",
       "      <td>3</td>\n",
       "      <td>PA</td>\n",
       "      <td>3</td>\n",
       "      <td>5.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  location_id  n_employees state  transaction_id  n_orders\n",
       "0        loc1            3    NY               1       2.0\n",
       "1        loc1            3    NY               2       3.0\n",
       "2        loc3            3    PA               3       5.0"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "orders_employees = locations_df.join(transactions_df, 'location_id')\n",
    "orders_employees.toPandas()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Feature columns\n",
    "- We need to create a feature column first"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "# this defines a transformer\n",
    "features_transformer = \\\n",
    "    feature.VectorAssembler(inputCols=['n_employees'], \n",
    "                            outputCol='features')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+-----------+-----+--------------+--------+--------+\n",
      "|location_id|n_employees|state|transaction_id|n_orders|features|\n",
      "+-----------+-----------+-----+--------------+--------+--------+\n",
      "|       loc1|          3|   NY|             1|     2.0|   [3.0]|\n",
      "|       loc1|          3|   NY|             2|     3.0|   [3.0]|\n",
      "|       loc3|          3|   PA|             3|     5.0|   [3.0]|\n",
      "+-----------+-----------+-----+--------------+--------+--------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "features_df = features_transformer.transform(orders_employees)\n",
    "features_df.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "# Now we can do the estimation\n",
    "lr_estimator = regression.LinearRegression(featuresCol='features',\n",
    "                                          labelCol='n_orders')\n",
    "lr_transformer = lr_estimator.fit(features_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "# make predictions\n",
    "lr_transformer.transform(features_df).toPandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "# coefficients\n",
    "print('intercept', lr_transformer.intercept)\n",
    "print('coeffs', lr_transformer.coefficients)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Activity\n",
    "1. Use Spark 1.6 to load and process `/datasets/income_data.txt` folder into an RDD of Rows\n",
    "1. Create a DataFrame from the RDD\n",
    "1. Use `LinearRegression` to estimate the relationship between `age`, `degree`, and `income`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Spark ML: Pipelines\n",
    "- Data science is all about building data analytic pipelines from raw data to models.\n",
    "- Spark ML Pipeline chains multiple Transformers and Estimators.\n",
    "- Pipelines can be saved and shared.\n",
    "- A Pipeline always starts as an Estimator that needs to be fitted.  \n",
    "\n",
    "<br>\n",
    "<center><img src=\"./images/unit-05/unit-05-0_as39.png\" width=\"100%\" align=\"center\"></center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Spark ML: Pipelines (2)\n",
    "- A Pipeline **always** needs to be fitted even when stages are all transformers.\n",
    "- A Pipeline transformer carries the entire process.    \n",
    "\n",
    "<br>\n",
    "<center><img src=\"./images/unit-05/unit-05-0_as40.png\" width=\"100%\" align=\"center\"></center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Spark ML: Algorithms\n",
    "- Some Estimators are algorithms that work on DataFrames.\n",
    "- For example, `LogisticRegression`.  \n",
    "\n",
    "<br>\n",
    "<center><img src=\"./images/unit-05/unit-05-0_as41.png\" width=\"100%\" align=\"center\"></center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Spark ML: Algorithms (2)\n",
    "- Algorithms can be part of Pipelines.\n",
    "- Pipelines can be combined with other Pipeline transformers.\n",
    "\n",
    "<br>\n",
    "<center><img src=\"./images/unit-05/unit-05-0_as42.png\" width=\"60%\" align=\"center\"></center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Lets try to standardize features from income prediction\n",
    "1. Use `feature.StandardScaler` to scale the features\n",
    "1. Put the vector assembler, scaler, and linear regression in one `Pipeline`\n",
    "1. Fit the new estimator (pipeline)\n",
    "1. Compare features with previous model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.ml import Pipeline\n",
    "# va = feature.VectorAssembler(???)\n",
    "# scaler = feature.StandardScaler(??)\n",
    "# regression = regression.LinearRegression(??)\n",
    "# pipe_estimator = Pipeline(stages=[va, scaler, regression])\n",
    "# pipe_model = pipe_estimator.fit(??)\n",
    "# extract last part of pipeline pipe_model.stages[-1]\n",
    "# and compare coefficients"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Summary\n",
    "- **Apache Spark** is a fast, in-memory computing system that runs on Hadoop.  \n",
    "\n",
    "- An application in Spark has several main components, including a driver program, worker nodes, executors and tasks.  \n",
    "\n",
    "- The `SparkContext` object is a key component of the driver program.  \n",
    "\n",
    "- An RDD is a fault-tolerant collection of elements that can be operated on in parallel.  \n",
    "\n",
    "- There are ways to create an RDD in Spark: the `parallelize` function or a reference to a file in HDFS, `textFile` and `sequenceFile`.  \n",
    "\n",
    "- There are two types of operations that can be done on RDDs: **transformations** and **actions**."
   ]
  }
 ],
 "metadata": {
  "celltoolbar": "Slideshow",
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
